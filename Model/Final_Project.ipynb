{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1a31C5J-X5Q6",
        "outputId": "db862dee-8ee2-488e-e31b-7e124a46890f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.65.0)\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2==3.0.1\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: scikit_learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.2) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.2) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.2) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.2) (3.1.0)\n",
            "Collecting sentence_transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers==2.2.2)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers==2.2.2)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers==2.2.2)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers==2.2.2) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers==2.2.2) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers==2.2.2) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers==2.2.2) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==2.2.2) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers==2.2.2) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers==2.2.2) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=c4d2d1400baee18de61ca47668fa46f280f274d64b0051cad4206ff1dfb02aa8\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.11)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.7.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.1.2)\n",
            "Installing collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.100.0 starlette-0.27.0\n",
            "Collecting uvicorn[standard]\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]) (8.1.4)\n",
            "Collecting h11>=0.8 (from uvicorn[standard])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard])\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard])\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]) (6.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard])\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard])\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard])\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from watchfiles>=0.13->uvicorn[standard]) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.0.0->watchfiles>=0.13->uvicorn[standard]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.0.0->watchfiles>=0.13->uvicorn[standard]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio>=3.0.0->watchfiles>=0.13->uvicorn[standard]) (1.1.2)\n",
            "Installing collected packages: websockets, uvloop, python-dotenv, httptools, h11, watchfiles, uvicorn\n",
            "Successfully installed h11-0.14.0 httptools-0.6.0 python-dotenv-1.0.0 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.6\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk==3.8.1\n",
        "!pip install numpy==1.24.3\n",
        "!pip install PyPDF2==3.0.1\n",
        "!pip install scikit_learn==1.2.2\n",
        "!pip install sentence_transformers==2.2.2\n",
        "!pip install fastapi\n",
        "!pip install uvicorn[standard]\n",
        "!pip install python-multipart\n",
        "!pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "import inflect\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "AkEsSdr-YLKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_single_document(file_path: str):\n",
        "    # Loads a single document from file path\n",
        "    if file_path[-4:] == '.txt':\n",
        "        with open(file_path, 'r') as f:\n",
        "            return f.read()\n",
        "\n",
        "    elif file_path[-4:] == '.pdf':\n",
        "        pdfFileObj = open(file_path, 'rb')\n",
        "        pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
        "        text = ''\n",
        "        for page in pdfReader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    elif file_path[-4:] == '.csv':\n",
        "        with open(file_path, 'r') as f:\n",
        "            return f.read()\n",
        "\n",
        "    else:\n",
        "        raise Exception('Invalid file type')\n",
        "\n",
        "\n",
        "def load_documents(file_paths: list[str] = None, source_dir: str = None):\n",
        "    # Loads all documents from source documents directory\n",
        "    if file_paths:\n",
        "        all_files = file_paths\n",
        "    elif source_dir:\n",
        "        all_files = [os.path.abspath(os.path.join(source_dir, file)) for file in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, file))]\n",
        "    else:\n",
        "        raise Exception('No file paths or source directory provided')\n",
        "\n",
        "    return [\n",
        "            {\n",
        "                'name': os.path.basename(file_path),\n",
        "                'content': load_single_document(f\"{file_path}\")\n",
        "            } for idx, file_path in enumerate(all_files) if file_path[-4:] in ['.txt', '.pdf', '.csv']\n",
        "        ]\n",
        "\n",
        "def load_io(file_byte = None):\n",
        "    # Loads a single document from file path\n",
        "    if file_byte.name[-3:] == 'txt':\n",
        "        return file_byte.read().decode(\"utf-8\")\n",
        "\n",
        "    elif file_byte.name[-3:] == 'pdf':\n",
        "        pdfReader = PyPDF2.PdfReader(file_byte)\n",
        "        text = ''\n",
        "        for page in pdfReader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    else:\n",
        "        raise Exception('Invalid file type')\n",
        "\n",
        "def load_btyes_io(files = None):\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            'name': file_btye.name,\n",
        "            'content': load_io(file_btye)\n",
        "        } for idx, file_btye in enumerate(files) if file_btye.name[-3:] in ['txt', 'pdf']\n",
        "    ]"
      ],
      "metadata": {
        "id": "SYNukfJeeTRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(documents, embedding='bert'):\n",
        "    if embedding == 'bert':\n",
        "        sbert_model = SentenceTransformer('bert-base-nli-mean-tokens', cache_folder=os.path.join(os.getcwd(), 'embedding'))\n",
        "\n",
        "        document_embeddings = sbert_model.encode(documents)\n",
        "        return document_embeddings\n",
        "\n",
        "    if embedding == 'minilm':\n",
        "        sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', cache_folder=os.path.join(os.getcwd(), 'embedding'))\n",
        "\n",
        "        document_embeddings = sbert_model.encode(documents)\n",
        "        return document_embeddings\n",
        "\n",
        "    if embedding == 'tfidf':\n",
        "        word_vectorizer = TfidfVectorizer(\n",
        "            sublinear_tf=True, stop_words='english')\n",
        "        word_vectorizer.fit(documents)\n",
        "        word_features = word_vectorizer.transform(documents)\n",
        "\n",
        "        return word_features\n"
      ],
      "metadata": {
        "id": "-sXH6ycpiEc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK pre processing"
      ],
      "metadata": {
        "id": "2NxKNvJkGxsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3POp_ciroWnT",
        "outputId": "4b09c453-5885-49d5-f599-b9e9e5a8f23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode(\n",
        "            'ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n"
      ],
      "metadata": {
        "id": "KDSTTcK4oW16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n"
      ],
      "metadata": {
        "id": "7Vl37w57oW5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        # print(word)\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n"
      ],
      "metadata": {
        "id": "KKI3DDfpoW9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words\n"
      ],
      "metadata": {
        "id": "-RHqkov_okjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(documents):\n",
        "    preprocessed_documents = []\n",
        "    for document in documents:\n",
        "        tokens = nltk.word_tokenize(document)\n",
        "        preprocessed = normalize(tokens)\n",
        "        preprocessed = ' '.join(map(str, preprocessed))\n",
        "        preprocessed_documents.append(preprocessed)\n",
        "\n",
        "    return preprocessed_documents"
      ],
      "metadata": {
        "id": "JQqT1P3bokmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6c_3KitBokqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(input_doc:str , ori_documents, embedding_type='bert'):\n",
        "    documents = np.array([doc['content'] for doc in ori_documents])\n",
        "    documents = np.insert(documents, 0, input_doc)\n",
        "    print(documents)\n",
        "    preprocessed_documents = preprocess(documents)\n",
        "    #print(preprocessed_documents)\n",
        "    print(\"Encoding with BERT...\")\n",
        "    documents_vectors = embedding(preprocessed_documents, embedding=embedding_type)\n",
        "    #print(documents_vectors)\n",
        "    print(\"Encoding finished\")\n",
        "\n",
        "    #compute cosine similarity\n",
        "    pairwise = cosine_similarity(documents_vectors)\n",
        "    #print(pairwise)\n",
        "    #only retain useful information\n",
        "    pairwise = pairwise[0,1:]\n",
        "    sorted_idx = np.argsort(pairwise)[::-1]\n",
        "    result_pairwise = pairwise[sorted_idx]\n",
        "\n",
        "    results = []\n",
        "    print('Resume ranking:')\n",
        "    for idx in sorted_idx:\n",
        "        single_result = {\n",
        "            'rank': idx,\n",
        "            'name': ori_documents[idx]['name'],\n",
        "            'similarity': pairwise[idx].item()\n",
        "        }\n",
        "        results.append(single_result)\n",
        "        print(f'Resume of candidite {idx}')\n",
        "        print(f'Cosine Similarity: {pairwise[idx]}\\n')\n",
        "\n",
        "    return results, result_pairwise"
      ],
      "metadata": {
        "id": "JpTw17o7okuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZvsNsgzokyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(query, files, embedding_type):\n",
        "\n",
        "    # pdfReader = PyPDF2.PdfReader(files[0])\n",
        "    # text = ''\n",
        "    # for page in pdfReader.pages:\n",
        "    #     text += page.extract_text()\n",
        "    # st.write(text)\n",
        "\n",
        "    results, _ = pipeline(query, load_btyes_io(files), embedding_type=embedding_type)\n",
        "    prob_per_documents = {result['name']: result['similarity'] for result in results}\n",
        "    return prob_per_documents\n",
        "\n",
        "sample_files = [\n",
        "    \"/content/resumes/Resume_Fernando_Hinojosa.pdf\",]\n",
        "\n",
        "sample_job_descriptions = {\n",
        "    \"Software Engineer\": \"\"\"We are looking for a software engineer with experience in Python and web development. The ideal candidate should have a strong background in building scalable and robust applications. Knowledge of frameworks such as Flask and Django is a plus. Experience with front-end technologies like HTML, CSS, and JavaScript is desirable. The candidate should also have a good understanding of databases and SQL. Strong problem-solving and communication skills are required for this role.\n",
        "    \"\"\",\n",
        "    \"Data Scientist\": \"\"\"We are seeking a data scientist with expertise in machine learning and statistical analysis. The candidate should have a solid understanding of data manipulation, feature engineering, and model development. Proficiency in Python and popular data science libraries such as NumPy, Pandas, and Scikit-learn is required. Experience with deep learning frameworks like TensorFlow or PyTorch is a plus. Strong analytical and problem-solving skills are essential for this position.\n",
        "    \"\"\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "Fqd-NBknpaZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    pipeline('''About Sleek\n",
        "\n",
        "Sleek is on a mission to revolutionize how entrepreneurs operate their business. We want to give small business owners peace of mind and the power of online solutions to allow them to focus on what they do best - growing their business. As we work for our thousands of customers, we gather millions of data points about their business, and in turn we transform those into useful, actionable insights and recommendations to accelerate their growth through smart algorithms.\n",
        "\n",
        "We are a team of 400 builders from 17 countries, with offices in Singapore, Philippines, Hong Kong, Australia and the UK committed to delivering a delightful experience to our clients!\n",
        "\n",
        "You will be working in the Data & Analytics organization to solve a wide range of business problems leveraging advanced analytics. You will deploy a flexible analytical skill set to deliver insightful data and analysis and model business scenarios. Your principal goal will be to use data to drive better business decisions. This means translating data into meaningful insights and recommendations and, where relevant, proactively implement improvements. You will be developing the business reporting and analysis for our internal operations world-wide. The job will require working closely with the various Business Units to understand their business question as well as the whole data team to understand and access available data.\n",
        "\n",
        "Position Duties\n",
        "Drive analytical problem-solving and deep dives. Work with large, complex data sets. Solve difficult, non-routine problems, applying advanced quantitative methods.\n",
        "Collaborate with a wide variety of cross-functional partners to determine business needs, drive analytical projects from start to finish.\n",
        "Align with involved stakeholders to set up dashboards and reports to drive data driven decision across all departments\n",
        "Working very closely with our Data team, Tech and Product team to understand the business logic to generate accurate reports and correct analysis\n",
        "\n",
        "Requirements\n",
        "Data Analysis\n",
        "Performance Standards\n",
        "Able to commit for a period of at least 4 months\n",
        "Currently pursuing a degree in Business Science, Engineering or relevant disciplines with a focus on data.\n",
        "Good knowledge in SQL, R and Python.\n",
        "Experience in data visualization tools (Tableau, PowerBI, Google DataStudio or equivalent) will be an added advantage.''',\n",
        "                   load_documents(source_dir = '/content/resumes'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8agop4qpag0",
        "outputId": "2b29d73a-9c89-4564-9b37-fd9385a2354a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['About Sleek\\n\\nSleek is on a mission to revolutionize how entrepreneurs operate their business. We want to give small business owners peace of mind and the power of online solutions to allow them to focus on what they do best - growing their business. As we work for our thousands of customers, we gather millions of data points about their business, and in turn we transform those into useful, actionable insights and recommendations to accelerate their growth through smart algorithms.\\n\\nWe are a team of 400 builders from 17 countries, with offices in Singapore, Philippines, Hong Kong, Australia and the UK committed to delivering a delightful experience to our clients!\\n\\nYou will be working in the Data & Analytics organization to solve a wide range of business problems leveraging advanced analytics. You will deploy a flexible analytical skill set to deliver insightful data and analysis and model business scenarios. Your principal goal will be to use data to drive better business decisions. This means translating data into meaningful insights and recommendations and, where relevant, proactively implement improvements. You will be developing the business reporting and analysis for our internal operations world-wide. The job will require working closely with the various Business Units to understand their business question as well as the whole data team to understand and access available data.\\n\\nPosition Duties\\nDrive analytical problem-solving and deep dives. Work with large, complex data sets. Solve difficult, non-routine problems, applying advanced quantitative methods.\\nCollaborate with a wide variety of cross-functional partners to determine business needs, drive analytical projects from start to finish.\\nAlign with involved stakeholders to set up dashboards and reports to drive data driven decision across all departments\\nWorking very closely with our Data team, Tech and Product team to understand the business logic to generate accurate reports and correct analysis\\n\\nRequirements\\nData Analysis\\nPerformance Standards\\nAble to commit for a period of at least 4 months\\nCurrently pursuing a degree in Business Science, Engineering or relevant disciplines with a focus on data.\\nGood knowledge in SQL, R and Python.\\nExperience in data visualization tools (Tableau, PowerBI, Google DataStudio or equivalent) will be an added advantag'\n",
            " 'Justine Debowski\\nSenior Insurance Data Analyst\\nProven senior insurance data analyst with 10+ years of experience leading\\ncutting-edge teams and analyzing small and large data sets. As a Lead IDA, I\\nspearheaded a highly skilled team that identiﬁed, recommended, and\\nimplemented continuous improvements based on gathered data. I believe that\\nmy experience in leading highly-effective teams and analyzing all types and\\nscales of data makes me an excellent candidate for this role at BitBucket.justinedeb@email.com\\n(123) 456-7890\\nDenver, CO\\ntwitter.com/just-deb\\n \\nWORK EXPERIENCE\\nPie Insurance\\nLead Insurance Data Analyst\\nDenver, CO|2019 - current\\nProduced and delivered 100+ BI dashboards and reports\\nMeasured and demonstrated organizational efﬁciencies with data\\nanalysis and reporting\\nWorked with 6 technical and business peers on development efforts,\\nincluding designing, analyzing, assessing, and reporting\\nManaged 100% of data management deliverables such as business need\\nanalysis and high-level data modeling techniques\\nIdentiﬁed, recommended, and implemented appropriate continuous\\nimprovement opportunities\\nArgo Group\\nData Analyst\\nDenver, CO|2015 - 2019\\nIdentiﬁed, analyzed, and interpreted trends and patterns in 260+\\ncomplex data sets\\nUsed statistical techniques to interpret data, analyze results and provide\\n30+ ongoing reports\\nDeveloped and implemented 3 new data collection systems and\\nmethodologies that optimized statistical efﬁciency and data quality\\nIdentiﬁed 8+ process gaps and worked cross-functionally to formulate\\ndata-driven improvement plans\\nIMA Financial Group, Inc.\\nData Analyst\\nDenver, CO|2012 - 2015\\nPerformed 80+ data proﬁling activities to provide statistics and\\ninformative summaries for integration of new source system data\\nBuilt out data dictionary with 320+ data deﬁnitions, values, and formats\\nCreated data collection mapping process for 3 systems, in collaboration\\nwith data stewards\\nInvestigated and documented data lineages throughout the data\\nlifecycle\\nCollaborated with 4 delivery teams in business, IT, data modeling, and\\ndata quality analytics to analyze market dataSKILLS\\nPython;SQL;Adobe Analytics\\n;STATA;Tableau;Excel;\\nAzure;Visio;Numpy;Pandas;\\nPowerBI;AWS\\nEDUCATION\\nBachelor of Science\\nData Analytics\\nColorado College\\n2008 - 2012\\nColorado Springs, CO\\nCERTIFICATIONS\\naCAP\\nAWS\\nIBM DSPC'\n",
            " 'Justine Debowski Senior Insurance Data Analyst Proven senior insurance data analyst with 2 years of experience leading cutting-edge teams and analyzing small and large data sets. As a Lead IDA, I spearheaded a highly skilled team that identiﬁed, recommended, and implemented continuous improvements based on gathered data. I believe that my experience in leading highly-effective teams and analyzing all types and scales of data makes me an excellent candidate for this role at BitBucket. justinedeb@email.com (123) 456-7890 Denver, CO twitter.com/just-deb \\n WORK EXPERIENCE Pie Insurance Lead Insurance Data Analyst Denver, CO | 2022 - current Produced and delivered 100+ BI dashboards and reports Measured and demonstrated organizational efﬁciencies with data analysis and reporting Worked with 6 technical and business peers on development efforts, including designing, analyzing, assessing, and reporting Managed 100% of data management deliverables such as business need analysis and high-level data modeling techniques Identiﬁed, recommended, and implemented appropriate continuous improvement opportunities  Argo Group Data Analyst Denver, CO | 2021 - 2022 Identiﬁed, analyzed, and interpreted trends and patterns in 260+ complex data sets Used statistical techniques to interpret data, analyze results and provide 30+ ongoing reports Developed and implemented 3 new data collection systems and methodologies that optimized statistical efﬁciency and data quality Identiﬁed 8+ process gaps and worked cross-functionally to formulate data-driven improvement plans  IMA Financial Group, Inc. Data Analyst Denver, CO | 2021 - 2021 Performed 80+ data proﬁling activities to provide statistics and informative summaries for integration of new source system data Built out data dictionary with 320+ data deﬁnitions, values, and formats Created data collection mapping process for 3 systems, in collaboration with data stewards Investigated and documented data lineages throughout the data lifecycle Collaborated with 4 delivery teams in business, IT, data modeling, and data quality analytics to analyze market data SKILLS Python; SQL; Adobe Analytics ; STATA; Tableau; Excel; Azure; Visio; Numpy; Pandas; PowerBI; AWS  EDUCATION Bachelor of Science Data Analytics Colorado College 2008 - 2012 Colorado Springs, CO  CERTIFICATIONS aCAP AWS IBM DSPC ']\n",
            "Encoding with BERT...\n",
            "Encoding finished\n",
            "Resume ranking:\n",
            "Resume of candidite 1\n",
            "Cosine Similarity: 0.8379244208335876\n",
            "\n",
            "Resume of candidite 0\n",
            "Cosine Similarity: 0.8137120604515076\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LKkDQgbpalg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}